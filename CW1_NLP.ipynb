{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mark Turos - Student Id: 9238806\n",
        "\n"
      ],
      "metadata": {
        "id": "a6xN1r5hvZEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlVo0wUFYcKh"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 1"
      ],
      "metadata": {
        "id": "eygdcVD4vnFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "filepath = 'lcp_single_train.tsv'\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "data = pd.read_csv(filepath, sep='\\t')\n",
        "data.head()\n",
        "\n",
        "## data preprocessing\n",
        "\n",
        "data['sentence'] = data['sentence'].str.lower()\n",
        "data['token'] = data['token'].str.lower()\n",
        "\n",
        "# feature extraction: token length, frequency\n",
        "data['token_length'] = data['token'].apply(len)\n",
        "token_counts = data['token'].value_counts()\n",
        "data['token_frequency'] = data['token'].map(token_counts)\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(data['sentence'])\n",
        "\n",
        "\n",
        "# combine TF-IDF + additional features\n",
        "additional_features = data[['token_length', 'token_frequency']].values\n",
        "features = np.hstack([tfidf_matrix.toarray(), additional_features])\n",
        "\n",
        "\n",
        "\n",
        "## modelling\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, data['complexity'], test_size=0.2, random_state=42)\n",
        "\n",
        "# feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# paramter tuning\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           n_jobs=-1,\n",
        "                           scoring='neg_mean_squared_error')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# cross-validation to check for overfitting\n",
        "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "cv_mean = np.mean(-cv_scores)\n",
        "cv_std = np.std(-cv_scores)\n",
        "print(f'Cross-validated MSE: {cv_mean:.4f} Â± {cv_std:.4f}')\n",
        "\n",
        "# training with the best parameters\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'R^2 Score: {r2:.4f}')\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------- #\n",
        "\n",
        "# test data\n",
        "test_file_path = 'lcp_single_test.tsv'\n",
        "test_data = pd.read_csv(test_file_path, sep='\\t')\n",
        "test_data['sentence'] = test_data['sentence'].str.lower()\n",
        "test_data['token'] = test_data['token'].str.lower()\n",
        "\n",
        "# feature Extraction\n",
        "test_data['token_length'] = test_data['token'].apply(len)\n",
        "\n",
        "token_counts = test_data['token'].value_counts()\n",
        "test_data['token_frequency'] = test_data['token'].map(token_counts)\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_matrix_test = vectorizer.transform(test_data['sentence'])\n",
        "additional_features_test = test_data[['token_length', 'token_frequency']].values\n",
        "test_features = np.hstack([tfidf_matrix_test.toarray(), additional_features_test])\n",
        "\n",
        "# scaling\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "\n",
        "# predictions\n",
        "test_predictions = best_model.predict(test_features_scaled)\n",
        "\n",
        "# assign predictions scores to test data\n",
        "test_data['predicted_complexity'] = test_predictions\n",
        "\n",
        "# download results to new .tsv\n",
        "test_data.to_csv('lcp_single_test_with_predictions.tsv', sep='\\t', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "WqI2EXuCZsm_",
        "outputId": "c7f743bd-2e10-4064-ff42-16a7367898e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/lcp_single_train.tsv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b552370af287>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/lcp_single_train.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/lcp_single_train.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 2"
      ],
      "metadata": {
        "id": "_NM45gDs06V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "train_file_path = 'lcp_multi_train.tsv'\n",
        "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
        "\n",
        "# preprocessing\n",
        "train_data['sentence'] = train_data['sentence'].str.lower()\n",
        "train_data['token'] = train_data['token'].str.lower()\n",
        "\n",
        "# feature extraction\n",
        "train_data['token_length'] = train_data['token'].apply(len)\n",
        "token_counts = train_data['token'].value_counts()\n",
        "train_data['token_frequency'] = train_data['token'].map(token_counts)\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(train_data['sentence'])\n",
        "additional_features = train_data[['token_length', 'token_frequency']].values\n",
        "features = np.hstack([tfidf_matrix.toarray(), additional_features])\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, train_data['complexity'], test_size=0.2, random_state=42)\n",
        "\n",
        "# feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# xgb model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# parameter tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=20,  # Number of parameter settings sampled\n",
        "                                   cv=5,\n",
        "                                   n_jobs=-1,\n",
        "                                   scoring='neg_mean_squared_error',\n",
        "                                   random_state=42)\n",
        "\n",
        "# fitting model w/ proper params\n",
        "random_search.fit(X_train, y_train)\n",
        "best_xgb_model = random_search.best_estimator_\n",
        "\n",
        "# model training\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# predictions on training data\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# eval\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'XGBoost - Mean Squared Error: {mse:.4f}')\n",
        "print(f'XGBoost - R^2 Score: {r2:.4f}')\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------- #\n",
        "\n",
        "# test data\n",
        "test_file_path = 'lcp_multi_test.tsv'\n",
        "test_data = pd.read_csv(test_file_path, sep='\\t')\n",
        "\n",
        "# data preprocessing and feature extraction + TFIDF\n",
        "test_data['sentence'] = test_data['sentence'].str.lower()\n",
        "test_data['token'] = test_data['token'].str.lower()\n",
        "\n",
        "test_data['token_length'] = test_data['token'].apply(len)\n",
        "token_counts = test_data['token'].value_counts()\n",
        "test_data['token_frequency'] = test_data['token'].map(token_counts)\n",
        "\n",
        "tfidf_matrix_test = vectorizer.transform(test_data['sentence'])\n",
        "additional_features_test = test_data[['token_length', 'token_frequency']].values\n",
        "test_features = np.hstack([tfidf_matrix_test.toarray(), additional_features_test])\n",
        "\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "\n",
        "# predictions\n",
        "test_predictions = best_xgb_model.predict(test_features_scaled)\n",
        "\n",
        "# assign predictions to the test data\n",
        "test_data['predicted_complexity'] = test_predictions\n",
        "\n",
        "# output resuilts to a new .tsv\n",
        "test_data.to_csv('lcp_multi_test_with_predictions.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFn1T3AEtOP1",
        "outputId": "36998671-e964-4446-992e-c7e5645b6e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost - Mean Squared Error: 0.0179\n",
            "XGBoost - R^2 Score: 0.2890\n",
            "Top 10 important features (indices): [1062 1337 1323  469 3233 3140 2492  442  307 3489]\n"
          ]
        }
      ]
    }
  ]
}