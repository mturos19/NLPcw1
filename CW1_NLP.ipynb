{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mark Turos - Student Id: 9238806\n",
        "\n"
      ],
      "metadata": {
        "id": "a6xN1r5hvZEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "UlVo0wUFYcKh"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 1"
      ],
      "metadata": {
        "id": "eygdcVD4vnFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "filepath = 'lcp_single_trial.tsv'\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "data = pd.read_csv(filepath, sep='\\t')\n",
        "data.head()\n",
        "\n",
        "## data preprocessing\n",
        "\n",
        "data['sentence'] = data['sentence'].str.lower()\n",
        "data['token'] = data['token'].str.lower()\n",
        "\n",
        "# feature extraction: token length, frequency\n",
        "data['token_length'] = data['token'].apply(len)\n",
        "token_counts = data['token'].value_counts()\n",
        "data['token_frequency'] = data['token'].map(token_counts)\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(data['sentence'])\n",
        "\n",
        "\n",
        "# combine TF-IDF + additional features\n",
        "additional_features = data[['token_length', 'token_frequency']].values\n",
        "features = np.hstack([tfidf_matrix.toarray(), additional_features])\n",
        "\n",
        "\n",
        "\n",
        "## modelling\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, data['complexity'], test_size=0.2, random_state=42)\n",
        "\n",
        "# feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# paramter tuning\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           n_jobs=-1,\n",
        "                           scoring='neg_mean_squared_error')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# training with the best parameters\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'R^2 Score: {r2:.4f}')\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------- #\n",
        "\n",
        "# test data\n",
        "test_file_path = 'lcp_single_test.tsv'\n",
        "test_data = pd.read_csv(test_file_path, sep='\\t')\n",
        "test_data['sentence'] = test_data['sentence'].str.lower()\n",
        "test_data['token'] = test_data['token'].str.lower()\n",
        "\n",
        "# feature Extraction\n",
        "test_data['token_length'] = test_data['token'].apply(len)\n",
        "\n",
        "token_counts = test_data['token'].value_counts()\n",
        "test_data['token_frequency'] = test_data['token'].map(token_counts)\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_matrix_test = vectorizer.transform(test_data['sentence'])\n",
        "additional_features_test = test_data[['token_length', 'token_frequency']].values\n",
        "test_features = np.hstack([tfidf_matrix_test.toarray(), additional_features_test])\n",
        "\n",
        "# scaling\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "\n",
        "# predictions\n",
        "test_predictions = best_model.predict(test_features_scaled)\n",
        "\n",
        "# assign predictions scores to test data\n",
        "test_data['predicted_complexity'] = test_predictions\n",
        "\n",
        "# download results to new .tsv\n",
        "test_data.to_csv('lcp_single_test_with_predictions.tsv', sep='\\t', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqI2EXuCZsm_",
        "outputId": "74d3fe9c-5363-4474-b4ce-79bde9b0da99"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.0163\n",
            "R^2 Score: 0.3495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subtask 2"
      ],
      "metadata": {
        "id": "_NM45gDs06V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training data\n",
        "train_file_path = 'lcp_multi_train.tsv'\n",
        "train_data = pd.read_csv(train_file_path, sep='\\t', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "# preprocessing\n",
        "train_data['sentence'] = train_data['sentence'].str.lower()\n",
        "train_data['token'] = train_data['token'].str.lower()\n",
        "\n",
        "# feature extraction\n",
        "train_data['token_length'] = train_data['token'].apply(len)\n",
        "token_counts = train_data['token'].value_counts()\n",
        "train_data['token_frequency'] = train_data['token'].map(token_counts)\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(train_data['sentence'])\n",
        "additional_features = train_data[['token_length', 'token_frequency']].values\n",
        "features = np.hstack([tfidf_matrix.toarray(), additional_features])\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, train_data['complexity'], test_size=0.2, random_state=42)\n",
        "\n",
        "# feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# xgb model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# parameter tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=20,  # Number of parameter settings sampled\n",
        "                                   cv=5,\n",
        "                                   n_jobs=-1,\n",
        "                                   scoring='neg_mean_squared_error',\n",
        "                                   random_state=42)\n",
        "\n",
        "# fitting model w/ proper params\n",
        "random_search.fit(X_train, y_train)\n",
        "best_xgb_model = random_search.best_estimator_\n",
        "\n",
        "# model training\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# predictions on training data\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# eval\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f'XGBoost - Mean Squared Error: {mse:.4f}')\n",
        "print(f'XGBoost - R^2 Score: {r2:.4f}')\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------- #\n",
        "\n",
        "# test data\n",
        "test_file_path = 'lcp_multi_test.tsv'\n",
        "test_data = pd.read_csv(test_file_path, sep='\\t', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "# data preprocessing and feature extraction + TFIDF\n",
        "test_data['sentence'] = test_data['sentence'].str.lower()\n",
        "test_data['token'] = test_data['token'].str.lower()\n",
        "\n",
        "test_data['token_length'] = test_data['token'].apply(len)\n",
        "token_counts = test_data['token'].value_counts()\n",
        "test_data['token_frequency'] = test_data['token'].map(token_counts)\n",
        "\n",
        "tfidf_matrix_test = vectorizer.transform(test_data['sentence'])\n",
        "additional_features_test = test_data[['token_length', 'token_frequency']].values\n",
        "test_features = np.hstack([tfidf_matrix_test.toarray(), additional_features_test])\n",
        "\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "\n",
        "# predictions\n",
        "test_predictions = best_xgb_model.predict(test_features_scaled)\n",
        "\n",
        "# assign predictions to the test data\n",
        "test_data['predicted_complexity'] = test_predictions\n",
        "\n",
        "# output resuilts to a new .tsv\n",
        "test_data.to_csv('lcp_multi_test_with_predictions.tsv', sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFn1T3AEtOP1",
        "outputId": "c5ae8af1-a75a-4379-cdb3-b1090efa3e97"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost - Mean Squared Error: 0.0182\n",
            "XGBoost - R^2 Score: 0.1671\n"
          ]
        }
      ]
    }
  ]
}